--- /usr/local/lib/python3.6/dist-packages/torch/nn/modules/activation.py
+++ /usr/local/lib/python3.6/dist-packages/torch/nn/modules/activation.py
@@ -1,7 +1,7 @@
 class ReLU(Module):
     r"""Applies the rectified linear unit function element-wise:
 
-    :math:`\text{ReLU}(x) = (x)^+ = \max(0, x)`
+    :math:`\text{ReLU}(x)= \max(0, x)`
 
     Args:
         inplace: can optionally do the operation in-place. Default: ``False``